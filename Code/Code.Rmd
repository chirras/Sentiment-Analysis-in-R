---
title: "Twitter Sentiment analysis on ‘Obamacare’"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages to be installed for the project

```{r, eval = FALSE}

install.packages("ROAuth")
install.packages("streamR")
install.packages("twitteR")
install.packages("dplyr")
install.packages("tm")
install.packages("wordcloud")
install.packages("tidytext")
install.packages("maps")
install.packages("RSQLite")
install.packages("janeaustenr")
install.packages("tidyr")
install.packages("RColorBrewer")
install.packages("ggplot2")
install.packages("reshape2")
install.packages("devtools")
install.packages("magrittr")
devtools::install_github("bmschmidt/wordVectors")

```

```{r, warning=FALSE, message=FALSE}

library(RCurl)
library(ROAuth)
library(streamR)
library(twitteR)
require(plyr)
require(dplyr)
require(stringr)
require(tm)
require(wordcloud)
require(ggplot2)
require(tidytext)
require(maps)
require(RSQLite)
require(openxlsx)
require(janeaustenr)
require(tidyr)
require(RColorBrewer)
require(ggplot2)
require(reshape2)
require(devtools)
library(wordVectors)
library(magrittr)

```


## Data Collection

# Connecting to twitter API

```{r}

# Downloading the certificate needed for authentication.
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")


# Setting the configuration details to authorize your application to access Twitter data
requestURL       <- "https://api.twitter.com/oauth/request_token"
accessURL        <- "https://api.twitter.com/oauth/access_token"
authURL          <- "https://api.twitter.com/oauth/authorize"
consumerKey      <- "GoXfUgH0mI24aXOYonCEAbAlw"
consumerSecret   <- "w2dLbb7qjkMfPtyvqhiufD9cVNDOO3qKKTxtv3svjWsTolbL2D"
accessToken      <- "2918042853-Yw7FPNxlg6M6PESqbHL9jJmbGH1TqVJz9obtEZ7"
accessTokenSecret<- "HsNYss4lsAcymWTVWFuOFIQj9qkQ2BaXj5PDyhXJZv7Bv"



# Authenticating user via OAuth handshake and saving the OAuth certificate to the local disk for future connections
my_oauth <- OAuthFactory$new( consumerKey=consumerKey,
                              consumerSecret=consumerSecret,
                              requestURL=requestURL,
                              accessURL=accessURL, 
                              authURL=authURL)
my_oauth$handshake(cainfo="cacert.pem")


# Registering credentials by setting up the OAuth credentials for a Twitter session
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)


```


# Extracting the tweets (Without Geolocations)

Tweets are pulled with out using any Geolocations that is not particulat to any location.


```{r}

list <- searchTwitter("obamacare", n=15000, lang = "en", resultType = "mixed") # includes popular + real time results
tweetsDF <- twListToDF(list)
comments <- as.list(tweetsDF$text)

```



# Extracting tweets with Geolocations

Geocodes for all the locations in USA are collected and mean value of both latitude, longitude are calculated with respect to each state.

```{r}

# Fetching the Geocodes into a data frame
locations <- read.xlsx("Avg Locations.xlsx")


# Average latitudes and logitudes for all the states with a radius of 1000 miles
lat <- locations$Latitude
long <- locations$Longitude
radius <- "1000mi"
  
# Generate data fram with longitude, latitude and chosen radius
coordinates<-as.data.frame(cbind(lat,long,radius))
coordinates$lat<-lat
coordinates$long<-long

# Create a string of the lat, long, and radius
for(i in 1:length(coordinates$lat)){
coordinates$search.twitter.entry[i]<-toString(c(coordinates$lat[i],
coordinates$long[i],radius))
}

# Take out spaces in the string
coordinates$search.twitter.entry<-gsub(" ","", coordinates$search.twitter.entry ,fixed=TRUE)

# Appending State
coordinates$state <- locations$State

# Extracting the tweets using the above Geocodes with a radius of 1000 miles.
tweetsLoc <- list()

for(i in 1:20)
{
  tweetsLoc[[i]] <- searchTwitter("#obamacare", n=500, lang = "en", resultType = "mixed", geocode = paste(locations$Latitude[i], locations$Longitude[i], "1000mi", sep = ",")) 
}

# Number of tweets with respective to the state
ind <- sapply(tweetsLoc, length)
coordinates$number.of.tweets <- ind

# Loading the tweets into a vector from the list
tweetsGeo <- c()
for(i in 1:length(tweetsLoc))
{
  if(ind[i] != 0)
  for(j in 1:ind[i])
  {
    tweetsGeo <- c(tweetsGeo, tweetsLoc[[i]][[j]])
  }
}


```


## Data Cleaning, Storing and Analysis

A similar method of Data cleaning, Storing and Analysis is followed for both tweets with Geo locations and without. 


```{r}

# Data Cleaning and Analysis for tweets with out Geo location.


# Creating a Corpus of data

tweets <- gsub("[^[:alnum:][:blank:]+?&@\\-]", "", comments)
tweets <- gsub("@\\w+","",tweets)
tweets <- gsub("http\\w+","",tweets)
crpus <- Corpus(VectorSource(tweets))
tweet <- tm_map(crpus, removeWords, c(stopwords("english"), "rt"))
tweet <- tm_map(tweet, removePunctuation)
tweet <- tm_map(tweet, content_transformer(tolower))
tweet <- tm_map(tweet, removeNumbers)
tweet <- tm_map(tweet, stripWhitespace)
tweet <- tm_map(tweet, removeWords, c("obamacare","trump"))


# Loading the cleansed tweets to a vector and further cleaing the data

tweetsCleaned <- vector()
for (i in 1:nrow(tweetsDF))
{
  tweetsCleaned[i] <- gettext(tweet[i])[1]
}

tweetsCleaned <- data.frame(trimws(tweetsCleaned, "both"))
names(tweetsCleaned) <- c("tweets")

tweetsCleaned$tweets <- gsub("rt+", "",tweetsCleaned$tweets)
tweetsCleaned$tweets <- trimws(tweetsCleaned$tweets, "both")


# Creating and Storing the data to SQLite database

# Creating and connecting to a Database
library(DBI)
db <- dbConnect(SQLite(), dbname = "FinalProject.sqlite")
summary(db)

# Creating a table in the database
dbWriteTable(db, "CLEANEDTWEETS", tweetsCleaned)
dbGetQuery(db, "Select count(*) from CLEANEDTWEETS")


# Using the dataframe for the Sentiment Analysis

# Uni-gram sentiment analysis

tidyWords <- tweetsCleaned %>%
  unnest_tokens(word, tweets)

tidyWords %>%
  right_join(get_sentiments("nrc"), by = "word") %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment, sort = TRUE)


# Sentiment Analysis at the tweet level

tidySentences <- tweetsCleaned %>%
  unnest_tokens(sentences, tweets, token = "lines")

tweetSentiment <- tidySentences %>%
  group_by(tweetID = row.names(tidySentences)) %>%
  unnest_tokens(word, sentences) %>%
  left_join(get_sentiments("afinn"), by = "word") %>%
  mutate(score = replace(score, is.na(score), 0)) %>%
  group_by(tweetID) 

tweetSentiment$tweetID <- as.numeric(tweetSentiment$tweetID)

score <- tweetSentiment %>% group_by(tweetID) %>%
  summarise(sentiment = sum(score)) %>%
  arrange(tweetID)


```



```{r}

# Data Cleaning and Analysis for tweets with Geo location.

# Building a corpus of data and cleaning the data using tm_map function

tweetGeo <- gsub("[^[:alnum:][:blank:]+?&@\\-]", "", comments)
tweetGeo <- gsub("@\\w+","",tweetGeo)
tweetGeo <- gsub("http\\w+","",tweetGeo)
crpus <- Corpus(VectorSource(tweetGeo))
tweetLoc <- tm_map(crpus, removeWords, c(stopwords("english"), "rt"))
tweetLoc <- tm_map(tweetLoc, removePunctuation)
tweetLoc <- tm_map(tweetLoc, content_transformer(tolower))
tweetLoc <- tm_map(tweetLoc, removeNumbers)
tweetLoc <- tm_map(tweetLoc, stripWhitespace)
tweetLoc <- tm_map(tweetLoc, removeWords, c("obamacare","trump"))


# Loading the cleansed tweets to a vector and further cleaing the data

tweetsCleanedGeo <- vector()
for (i in 1:length(tweetsGeo))
{
  tweetsCleanedGeo[i] <- gettext(tweetLoc[i])[1]
}

tweetsCleanedGeo <- data.frame(trimws(tweetsCleanedGeo, "both"))
names(tweetsCleanedGeo) <- c("tweets")

tweetsCleanedGeo$tweets <- gsub("rt+", "",tweetsCleanedGeo$tweets)
tweetsCleanedGeo$tweets <- trimws(tweetsCleanedGeo$tweets, "both")


# Creating and Storing the data to SQLite database

# Creating a table in the database
dbWriteTable(db, "CLEANEDTWEETSGEO", tweetsCleanedGeo)
dbGetQuery(db, "Select count(*) from CLEANEDTWEETSGEO")


# Using the dataframe for the Sentiment Analysis

# Uni-Gram Analysis

tidyWordsGeo <- tweetsCleanedGeo %>%
  unnest_tokens(word, tweets)

tidyWordsGeo %>%
  right_join(get_sentiments("nrc"), by = "word") %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment, sort = TRUE)

# Tweet sentiment

tidySentencesGeo <- tweetsCleanedGeo %>%
  unnest_tokens(sentences, tweets, token = "lines")


tweetSentimentGeo <- tidySentencesGeo %>%
  group_by(tweetID = row.names(tidySentencesGeo)) %>%
  unnest_tokens(word, sentences) %>%
  left_join(get_sentiments("afinn"), by = "word") %>%
  mutate(score = replace(score, is.na(score), 0)) %>%
  group_by(tweetID) 

tweetSentimentGeo$tweetID <- as.numeric(tweetSentimentGeo$tweetID)

scoreGeo <- tweetSentimentGeo %>% group_by(tweetID) %>%
  summarise(sentiment = sum(score)) %>%
  arrange(tweetID)



```


# Data Visualization

Generating a word cloud with a minimum frequency of words equal to 50

```{r}
# Data Visualization for the tweets without Geocodes
# Generating a word cloud

wordList <- str_split(tweet, '\\s+')
words <- unlist(wordList)

wordcloud(words, max.words = 200,min.freq=50,scale=c(3,0.5), 
          random.order = FALSE,rot.per=.5,vfont=c("sans serif","plain"),colors=palette())


# Most common positive and negative words

commonWords <- tidyWords %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


commonWords %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()


# Overall sentiment

ggplot(data = score, mapping = aes(x = tweetID, y = sentiment)) + geom_bar(alpha = 0.7, stat = "identity", show.legend = FALSE) 



```



```{r}
# Data Visualization for the tweets with Geocodes
# Generating a word cloud with positive and negative words

tidyWordsGeo %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 200)


# Most common positive and negative words

bing_word_counts <- tidyWordsGeo %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()


bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()


# Overall sentiment

ggplot(data = scoreGeo, mapping = aes(x = tweetID, y = sentiment)) + geom_bar(alpha = 0.7, stat = "identity", show.legend = FALSE) 


# making the US map indicating number of tweets from each state.
all_states <- map_data("state")

# plot all points on the map
stateTweetNum <- ggplot()
stateTweetNum <- stateTweetNum + geom_polygon(data=all_states, aes(x=long, y=lat, group = group),colour="black", fill=NA)

myPalette <- colorRampPalette(rev(brewer.pal(11, "Spectral")))
sc <- scale_colour_gradientn(colours = myPalette(100), limits=c(1, max(ind)))

stateTweetNum <- stateTweetNum + geom_point(data=coordinates, aes(x=long, y=lat,color=number.of.tweets)) + scale_size(name="number.of.tweets") + sc + xlim(-130,-65) + ylim(25,50)

stateTweetNum


```



## Data Exploration Code

```{r}

lapply(tweet, write, "tweets.txt", append=TRUE)
model <- train_word2vec("tweets.txt", output_file = "tweets.bin", vectors = 300, threads = 1)


model[["repeal"]]
model %>% nearest_to(model[["repeal"]] %>% round(3))


repealWords <- model %>% nearest_to(model[[c("repeal","revoke","reverse","reversal","vacate","nullify","abolish","revocation")]], 50) %>% names()
sample(repealWords,50)


repeal <- model[rownames(model) %in% repealWords[1:50],]
repealDistance <- cosineDist(repeal, repeal) %>% as.dist
plot(as.dendrogram(hclust(repealDistance)), horiz=F, cex=1, main="Cluster Dendrogram of fifty words near to Repeal")


goodScore <- model %>% cosineSimilarity(model[[c("good","great","happy","greatest","suitable","fit","appropriate","substantial","agreeable","success","successful","potent","practical","essential","survived","repealrepublicans","trumpcarefail")]])
badScore <- model %>% cosineSimilarity(model[[c("bad","expensive","costly","negative","unfit","failure","fail","worse","struggle","repealobamacare")]])


plot(goodScore, badScore, type='n', main = "Top words plotted by their similarity to positive or negative connotation")
text(goodScore, badScore, labels = rownames(model), cex = .7)
abline(a=0,b=1)


```


























